

from pyspark.sql.functions import *

# Azure Event Hub Configuration
event_hub_namespace = "<<Event hub Namespace>>.servicebus.windows.net"
event_hub_name="<<Event hub Name>>"  
event_hub_conn_str = "<<connection_details>>"

kafka_options = {
    'kafka.bootstrap.servers': f"{event_hub_namespace}:9093",
    'subscribe': event_hub_name,
    'kafka.security.protocol': 'SASL_SSL',
    'kafka.sasl.mechanism': 'PLAIN',
    'kafka.sasl.jaas.config': f'kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username="$ConnectionString" password="{event_hub_conn_str}";',
    'startingOffsets': 'latest',
    'failOnDataLoss': 'false'
}


#Read Data from event hub

raw_df = (
            spark.readStream
            .format("kafka")
            .options(**kafka_options)
            .load()
        )

#cast data to json
json_df = raw_df.selectExpr("CAST(value AS STRING) as raw_json")

bronze_path = "abfss://bronze@stcareflow360.dfs.core.windows.net/patient_flow"

json_df.writeStream \
.format("delta") \
.outputMode("append") \
.option("checkpointLocation", bronze_path + "_checkpoints/patient_flow") \
.start(bronze_path)
